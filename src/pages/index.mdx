---
layout: ../layouts/Layout.astro
title: A Wearable Thumb Device for Fruit Firmness Estimation with Vision-Based Tactile Sensing
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: ihlab_logo.jpeg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import short_demo from "../assets/short_demo.mp4";
import main_fig from "../assets/main.png";
import wear_fig from "../assets/wear.png";
import explod_fig from "../assets/exploded.png";
import comp_fig from "../assets/comparison.png";
import data_fig from "../assets/data.png";
import frame_fig from "../assets/framework.png";
import r1_fig from "../assets/r1.png";
import r2_fig from "../assets/r2.png";

import Splat from "../components/Splat.tsx"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Mashood M. Mohsan",
      url: "https://scholar.google.com/citations?user=WMcSpaAAAAAJ&hl=en",
      notes: ["*", "1"],
    },
    {
      name: "Basma Hasanen",
      url: "https://scholar.google.com/citations?user=6ixcL4cAAAAJ&hl=en",
      notes: ["*", "1"],
    },
    {
      name: "Taimur Hassan",
      url: "https://scholar.google.com.pk/citations?user=11mwy0YAAAAJ&hl=en",
      notes: ["2"],
    },
    {
      name: "Lakmal Seneviratne",
      url: "https://scholar.google.com/citations?user=MIqCjoIAAAAJ&hl=en",
      notes: ["1"],
    },
    {
      name: "Irfan Hussain",
      url: "https://scholar.google.com/citations?user=bCC3kdUAAAAJ&hl=en",
      notes: ["1"],
    }
  ]}

  notes={[
    {
      symbol: "*",
      text: "Equal Contribution",
    },
    {
      symbol: "1",
      text: "Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, UAE",
    },
    {
      symbol: "2",
      text: "Department of Electrical, Computer and Biomedical Engineering, Abu Dhabi University, UAE",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://mashood3624.github.io/WearableDevice/paper.pdf",
      icon: "ri:file-pdf-2-line",
    },
    {
    name: "DOI",
    url: "https://doi.org/10.1016/j.compag.2025.110593",
    icon: "ri:link",
    },  
    {
      name: "Code",
      url: "https://github.com/Mashood3624/WearableDevice",
      icon: "ri:github-line",
    },
    {
      name: "Youtube",
      url: "https://youtu.be/MaLs7fFNrok?si=vRf5nCoIOY40dm-0",
      icon: "ri:youtube-line"
    }
  ]}
  />

<Video source={short_demo} />

{/* <HighlightedSection> */}

<h2 style={{ textAlign: "center" }}>Abstract</h2>

<div style={{ textAlign: "justify" }}>
Recent advancements in non-destructive technologies have enabled precise firmness measurement for various fruits, including kiwifruit. However, existing methods remain limited by high costs, environmental sensitivity, and field application impracticality. This work introduces <strong>a novel wearable device for estimating non-destructive fruit firmness</strong>, combining human tactile interaction with vision-based tactile sensing and edge computing. Worn on the thumb, the device leverages embodied intelligence, merging intuitive human touch with the precision of a vision-based tactile sensor. A single-board computer processes tactile images locally, enabling reliable operation even in remote environments. The device employs our proposed deep learning model for real-time firmness predictions from a single palpation, minimizing repetitive handling and reducing fruit bruising. Its ergonomic, symmetrical design supports comfortable use on either hand, enhancing usability. Compact and portable, the device integrates essential components within a housing <strong>measuring 40 mm × 25 mm × 72 mm and weighing only 135 g</strong>. Validated through non-destructive ripeness assessments on 'Hayward' Kiwifruit, the device demonstrated a strong correlation between tactile images and firmness values when paired with our proposed model, achieving <strong> a coefficient of determination (R<sup>2</sup>) of 0.89</strong>. This study created a dedicated dataset on Kiwi firmness to support model development and validation.
</div>
{/* </HighlightedSection> */}

<Figure>
  <Image slot="figure" source={main_fig} altText="Diagram of the transformer deep learning architecture." />
  <span slot="caption">The proposed wearable device for real-time, and non-destructive fruit firmness estimation. The user palpates a Kiwi, and
the proposed model processes the VBTS palpation recording to predict firmness in a non-destructive approach.</span>
</Figure>

## Comparison with Conventional Firmness Testing Methods

<div style={{ textAlign: "justify" }}>For fruits that do not exhibit significant color changes during ripening, firmness serves as a key indicator of ripeness. Existing methods for firmness assessment include mechanical (rupture and durometer), optical, and vibrational techniques. However, these approaches are often invasive or influenced by environmental factors. Our device integrates human dexterity with AI-driven tactile sensing, offering non-destructive, real-time firmness estimation with improved adaptability and accuracy.</div>
<Figure>
  <Image slot="figure" source={comp_fig} altText="Diagram of the transformer deep learning architecture." />
</Figure>

## Development of Our Device

<div style={{ textAlign: "justify" }}>The device comprises two modules: a thumb unit with a vision-based tactile sensor (VBTS) and a wrist unit housing the control board and user interface in a 3D-printed casing. Adjustable elastic straps ensure comfort and stability. The hardware integrates a Radxa Zero 2 Pro, OLED display, single button, DIGIT sensor, and is powered by a 15W external power bank. A CAD exploded view highlights its modular internal design.</div>
<TwoColumns>
  <Figure slot="left">
    <Image slot="figure" source={wear_fig} altText="Diagram of the transformer deep learning architecture." />
  </Figure>
  <Figure slot="right">
    <Image slot="figure" source={explod_fig} altText="Diagram of the transformer deep learning architecture." />
  </Figure>
</TwoColumns>

## Dataset Collection
<div style={{ textAlign: "justify" }}>A dataset of 530 paired samples was collected from 106 randomly selected kiwifruits using our
device. Each fruit was palpated at five points (P1–P5) to capture regional firmness variations,
while ground truth was obtained at two equatorial points (P1–P2 or P3–P4) using a penetrometer. 
Although soft (0.5 kg/cm²) and firm (3.15 kg/cm²) fruits appeared visually similar, their VBTS 
signatures differed significantly.</div>

<Figure>
  <Image slot="figure" source={data_fig} altText="Diagram of the transformer deep learning architecture." />
</Figure>

## Proposed Architecture

<div style={{ textAlign: "justify" }}>VBTS palpation recordings over n frames are processed by a CNN-based video encoder to extract spatial features. These are linearly projected into 1D representations at each time step and passed to an LSTM to capture temporal dependencies. The LSTM output is fed into a fully connected layer to predict fruit firmness.</div>

<Figure>
  <Image slot="figure" source={frame_fig} altText="Diagram of the transformer deep learning architecture." />
</Figure>

## Results

<div style={{ textAlign: "justify" }}>The proposed model demonstrated a strong correlation between predicted and reference firmness values, achieving an R² score of 0.89 (Figure, left). The residual analysis (Figure, right) shows a random scatter around zero, indicating no systematic bias and suggesting reliable estimation performance across the entire firmness range.</div>

<TwoColumns>
  <Figure slot="left">
    <Image slot="figure" source={r1_fig} altText="Diagram of the transformer deep learning architecture." />
  </Figure>
  <Figure slot="right">
    <Image slot="figure" source={r2_fig} altText="Diagram of the transformer deep learning architecture." />
  </Figure>
</TwoColumns>

## Our Related work 

This project builds on our prior research in tactile sensing and fruit firmness estimation. For further exploration of our methods and insights, refer to:

- [SwishFormer for robust firmness and ripeness recognition of fruits using visual tactile imagery (Postharvest Biology and Technology, 2025)](https://mashood3624.github.io/SwishFormer/): Robot palpation-based fruit sorting is performed using the proposed SwishFormer algorithm, which processes vision-based tactile sequences via a hardswish-activated transformer and Random Forest regressor. The system achieves a 96.6–98.3% ripeness classification success and up to 13.69% R² improvement with 2× fewer parameters than prior models.
- [Soft Vision-Based Tactile-Enabled SixthFinger: Advancing Daily Objects Manipulation for Stroke Survivors (RoboSoft, 2025)](https://arxiv.org/abs/2501.06806): Presents a soft robotic finger with integrated vision-based tactile sensing for autonomous grip adjustment via slippage detection. A transformer-based model trained on diverse tactile data enables robust object manipulation for stroke survivors across varied real-world tasks.
- [Cross-Modal Knowledge Distillation for Efficient Material Recognition: Aligning Language Descriptions with Tactile Image Models (IEEE IROS WorkShop BoB, 2024)](https://openreview.net/forum?id=EKYZaxzvae): Proposes a cross-modal distillation framework where a BART language model transfers semantic knowledge to a ViT-based tactile image model for material recognition. The distilled ViT achieves 74.70% accuracy (↑16.87%) and is validated through real-world UR10 robot experiments.

We encourage readers to explore these works for deeper technical context and complementary advancements.

## Acknowledgements
This publication is based upon work supported by the Khalifa University of Science and Technology under Award No. RC1-2018-KUCARS. 
Some elements of this project's README design were adapted from <a href="https://github.com/rpl-cmu/YCB-Slide"><b>YCB-Slide</b></a>.
The website was built using  <a href="https://github.com/RomanHauksson/academic-project-astro-template"><b>Roman Hauksson's</b></a> academic project page template.

## BibTeX citation

```bibtex
@article{MOHSAN2025110593,
title = {A wearable thumb device for fruit firmness estimation with vision-based tactile sensing},
journal = {Computers and Electronics in Agriculture},
volume = {237},
pages = {110593},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2025.110593},
url = {https://www.sciencedirect.com/science/article/pii/S0168169925006994},
author = {Mashood M. Mohsan and Basma B. Hasanen and Taimur Hassan and Lakmal Seneviratne and Irfan Hussain}
}
```